{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5Jij9VuCgTx"
   },
   "source": [
    "## **Classification of Mental Health Disorder with Emotion Features from social media Text Using Machine Learning Methods**\n",
    "\n",
    "*  Import Libraries\n",
    "*  Dataset Upload\n",
    "*  Data Cleaning and Preprocessing\n",
    "*  EDA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1f8KZtEpCwzS"
   },
   "source": [
    "#**Import Python Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 89962,
     "status": "ok",
     "timestamp": 1692914589336,
     "user": {
      "displayName": "Oyenike Akanji",
      "userId": "06472967511729576704"
     },
     "user_tz": -60
    },
    "id": "1coZx4ddrpw4"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install texthero\n",
    "!pip install -U spacy\n",
    "!python -m spacy download en_core_web_sm\n",
    "!pip install langdetect\n",
    "!pip install contractions\n",
    "!pip install beautifulsoup4\n",
    "!pip install unidecode\n",
    "!pip install transformers\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 1483,
     "status": "ok",
     "timestamp": 1692914822047,
     "user": {
      "displayName": "Oyenike Akanji",
      "userId": "06472967511729576704"
     },
     "user_tz": -60
    },
    "id": "nZzPvO5rrpw6"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import re\n",
    "import warnings\n",
    "# Disable warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gensim\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import spacy\n",
    "#import texthero as hero\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from wordcloud import WordCloud, ImageColorGenerator\n",
    "from PIL import Image\n",
    "from gensim import corpora\n",
    "import contractions\n",
    "from bs4 import BeautifulSoup\n",
    "from textblob import TextBlob\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "# Download required nltk data\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xn4dgMCbFUnT"
   },
   "source": [
    "# **Dataset Upload**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1692914825963,
     "user": {
      "displayName": "Oyenike Akanji",
      "userId": "06472967511729576704"
     },
     "user_tz": -60
    },
    "id": "eYMjti4z3U_I"
   },
   "outputs": [],
   "source": [
    "#Load dataset from Googledrive\n",
    "\n",
    "data_path = 'https://docs.google.com/spreadsheets/d/e/2PACX-1vTJ5zPd_9pGT5V97N5O0-jkNIhj2h3WaIObTKps4P8KmBoE0o6F9cMCb8XuCW0CjkCAMuGOG3LJPdPu/pub?gid=1387298660&single=true&output=csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MEuj6AG85GGU"
   },
   "outputs": [],
   "source": [
    "# Read data from csv file\n",
    "dataset = pd.read_csv(data_path)\n",
    "dataset.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T-lL9jwVt7TY"
   },
   "source": [
    "## **Data Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YNIDKGYIIZKP"
   },
   "source": [
    "**Data Cleaning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "82_DGkcSxraC"
   },
   "outputs": [],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d1a60a79"
   },
   "outputs": [],
   "source": [
    "#Check null values\n",
    "dataset.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "15ea4c7e"
   },
   "outputs": [],
   "source": [
    "#Remove any row with null values\n",
    "dataset = dataset.dropna(how='any')\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1JzUavD0O4a2"
   },
   "outputs": [],
   "source": [
    "#Check the count of the target labels\n",
    "dataset['subreddit'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e59c74db"
   },
   "outputs": [],
   "source": [
    "#Remove unnecessary Labels\n",
    "labels_to_keep = ['depression', 'Anxiety', 'bipolar', 'BPD', 'ADHD', 'autism']\n",
    "\n",
    "# Filter rows based on labels\n",
    "dataset = dataset[dataset['subreddit'].isin(labels_to_keep)]\n",
    "dataset['subreddit'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DlHuS3q14FR_"
   },
   "outputs": [],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EusRTd9crpw6"
   },
   "outputs": [],
   "source": [
    "# Drop rows with removed post\n",
    "dataset.drop(dataset[(dataset['title'] =='\\\\[removed\\\\]')].index, inplace=True)\n",
    "dataset.drop(dataset[(dataset['title'] =='[removed]')].index, inplace=True)\n",
    "dataset.drop(dataset[(dataset['subreddit'] =='\\\\[removed\\\\]')].index, inplace=True)\n",
    "dataset.drop(dataset[(dataset['subreddit'] =='[removed]')].index, inplace=True)\n",
    "dataset.drop(dataset[(dataset['body'] =='\\\\[removed\\\\]')].index, inplace=True)\n",
    "dataset.drop(dataset[(dataset['body'] =='[removed]')].index, inplace=True)\n",
    "dataset.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1692914850942,
     "user": {
      "displayName": "Oyenike Akanji",
      "userId": "06472967511729576704"
     },
     "user_tz": -60
    },
    "id": "jzTiaNTn5FB4"
   },
   "outputs": [],
   "source": [
    "# Rename labels\n",
    "dataset['subreddit'] = dataset['subreddit'].replace({\n",
    "    'depression': 'Depression',\n",
    "    'autism': 'Autism',\n",
    "    'bipolar': 'Bipolar'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GZxo6M0F65Bi"
   },
   "outputs": [],
   "source": [
    "#view new labels count\n",
    "dataset['subreddit'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u9yTXPQQIUv_"
   },
   "source": [
    "# **Exploratory Data Analysis (EDA)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tvh1fkdD7UEy"
   },
   "outputs": [],
   "source": [
    "#Distribution of subreddit categories\n",
    "labels = ['ADHD','Depression','Anxiety', 'BPD', 'Austism', 'Bipolar']\n",
    "sizes = [24554, 19021, 11882, 9773, 7510, 4738 ]\n",
    "custom_colours = ['g', 'b', 'r', 'c', 'm', 'y']\n",
    "\n",
    "plt.figure(figsize=(20, 6), dpi=227)\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.pie(sizes, labels=labels, textprops={'fontsize': 10}, startangle=140,\n",
    "        autopct='%1.0f%%', colors=custom_colours, explode=[0, 0, 0, 0, 0, 0.05])\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.barplot(x=labels, y=sizes)\n",
    "\n",
    "plt.ylabel('Counts')  # Add y-axis label\n",
    "\n",
    "# Add label values on top of the bars\n",
    "for i, v in enumerate(sizes):\n",
    "    plt.text(i, v + 1000, str(v), ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9E6kL4_Ycb-L"
   },
   "outputs": [],
   "source": [
    "#Join the Title and Text column to form a sentence\n",
    "dataset['post'] = dataset['title'] + ' ' + dataset['body']\n",
    "dataset.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bv-eTtMk_tEa"
   },
   "outputs": [],
   "source": [
    "#Drop irrelevant columns\n",
    "columns_to_drop = ['title', 'body']\n",
    "dataset = dataset.drop(columns_to_drop, axis=1)\n",
    "dataset.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 4997,
     "status": "ok",
     "timestamp": 1692914860923,
     "user": {
      "displayName": "Oyenike Akanji",
      "userId": "06472967511729576704"
     },
     "user_tz": -60
    },
    "id": "-SpQIWKDJYen"
   },
   "outputs": [],
   "source": [
    "#Check the the lenth of posts\n",
    "dataset['total_words'] = dataset['post'].apply(lambda x: len(x.split()))\n",
    "\n",
    "def count_total_words(text):\n",
    "    char = 0\n",
    "    for word in text.split():\n",
    "        char += len(word)\n",
    "    return char\n",
    "\n",
    "dataset['total_char'] = dataset[\"post\"].apply(count_total_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ve5DtiGwKQMA"
   },
   "outputs": [],
   "source": [
    "dataset.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qTfkJrp5gVOl"
   },
   "source": [
    "### **Word Count**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-03T18:48:44.855425Z",
     "iopub.status.busy": "2023-03-03T18:48:44.853609Z",
     "iopub.status.idle": "2023-03-03T18:48:45.032526Z",
     "shell.execute_reply": "2023-03-03T18:48:45.030882Z",
     "shell.execute_reply.started": "2023-03-03T18:48:44.855357Z"
    },
    "executionInfo": {
     "elapsed": 3804,
     "status": "ok",
     "timestamp": 1692914866434,
     "user": {
      "displayName": "Oyenike Akanji",
      "userId": "06472967511729576704"
     },
     "user_tz": -60
    },
    "id": "XJN_SUHaxCtz"
   },
   "outputs": [],
   "source": [
    "# Concatenate all posts into a single string\n",
    "dataset['post'] = dataset['post'].astype(str)\n",
    "total_words = ' '.join(dataset['post'].values)\n",
    "# Remove URLs, mentions, and hashtags from the text\n",
    "total_words = re.sub(r'http\\S+', '', total_words)\n",
    "total_words = re.sub(r'@\\S+', '', total_words)\n",
    "total_words = re.sub(r'#\\S+', '', total_words)\n",
    "# Split the text into individual words\n",
    "words = total_words.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-03T18:48:45.034433Z",
     "iopub.status.busy": "2023-03-03T18:48:45.034063Z",
     "iopub.status.idle": "2023-03-03T18:48:45.270368Z",
     "shell.execute_reply": "2023-03-03T18:48:45.268588Z",
     "shell.execute_reply.started": "2023-03-03T18:48:45.034397Z"
    },
    "executionInfo": {
     "elapsed": 6026,
     "status": "ok",
     "timestamp": 1692914872459,
     "user": {
      "displayName": "Oyenike Akanji",
      "userId": "06472967511729576704"
     },
     "user_tz": -60
    },
    "id": "SB8c7iN_xCt0"
   },
   "outputs": [],
   "source": [
    "# Remove stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "words = [word for word in words if not word in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-03T18:48:45.272220Z",
     "iopub.status.busy": "2023-03-03T18:48:45.271915Z",
     "iopub.status.idle": "2023-03-03T18:48:45.552609Z",
     "shell.execute_reply": "2023-03-03T18:48:45.550889Z",
     "shell.execute_reply.started": "2023-03-03T18:48:45.272190Z"
    },
    "id": "tm0mqzkhxCt0"
   },
   "outputs": [],
   "source": [
    "# Count the frequency of each word\n",
    "word_counts = Counter(words)\n",
    "top_words = word_counts.most_common(25)\n",
    "top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IXI_ooy_knuP"
   },
   "outputs": [],
   "source": [
    "# Create a bar chart of the most common words\n",
    "top_words = word_counts.most_common(10)  # Change the number to show more/less words\n",
    "x_values = [word[0] for word in top_words]\n",
    "y_values = [word[1] for word in top_words]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "bars = ax.bar(x_values, y_values)\n",
    "plt.xlabel('Word')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Top 10 Commonly Used Words')\n",
    "\n",
    "# Add value labels on top of the bars\n",
    "for bar in bars:\n",
    "    yval = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, yval, round(yval, 2), ha='center', va='bottom', color='black')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 9534,
     "status": "ok",
     "timestamp": 1692914883508,
     "user": {
      "displayName": "Oyenike Akanji",
      "userId": "06472967511729576704"
     },
     "user_tz": -60
    },
    "id": "820367d6",
    "outputId": "18be0446-ae9c-46e9-d9f5-aa77d1171864"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words: 14398634 | Unique words: 225273\n"
     ]
    }
   ],
   "source": [
    "# Print the number of words and unique words\n",
    "unique_words = set(total_words.lower().split())\n",
    "print(f\"Total words: {len(total_words.split())} | Unique words: {len(unique_words)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Va8wJVxvK-8T"
   },
   "outputs": [],
   "source": [
    "#Kdeplot of the number of word of 'post' by 'subreddit'\n",
    "fig, ax = plt.subplots(1, 2, figsize=(20, 6))\n",
    "\n",
    "sns.kdeplot(x = dataset['total_words'], hue= dataset['subreddit'],  ax=ax[0], label='post lenght')\n",
    "ax[0].set_title('Distribution of word count by Subreddit')\n",
    "\n",
    "sns.kdeplot(x = dataset['total_char'], hue= dataset['subreddit'],  ax=ax[1], label='text word')\n",
    "ax[1].set_title('Distribution of character count by Subreddit')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z9fWLLRyDXDJ"
   },
   "source": [
    "# **Data Pre-processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "executionInfo": {
     "elapsed": 4047,
     "status": "ok",
     "timestamp": 1692914891014,
     "user": {
      "displayName": "Oyenike Akanji",
      "userId": "06472967511729576704"
     },
     "user_tz": -60
    },
    "id": "UjaQbYwxrpw7"
   },
   "outputs": [],
   "source": [
    "load_model = spacy.load('en_core_web_sm', disable=[\"parser\", \"ner\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "executionInfo": {
     "elapsed": 341,
     "status": "ok",
     "timestamp": 1692914918292,
     "user": {
      "displayName": "Oyenike Akanji",
      "userId": "06472967511729576704"
     },
     "user_tz": -60
    },
    "id": "reIWyIb9rpw7"
   },
   "outputs": [],
   "source": [
    "#Create functions for data cleaning and preprocessing\n",
    "\n",
    "def lemmatization(text):\n",
    "    doc = load_model(text)\n",
    "    return \" \".join([token.lemma_ for token in doc])\n",
    "\n",
    "def remove_url(text):\n",
    "    return re.sub(r'https?://\\S+|www\\.\\S+', '', str(text))\n",
    "\n",
    "def remove_newline(text):\n",
    "    return ' '.join(text.split())\n",
    "\n",
    "def convert_lowercase(text):\n",
    "    text = text.lower()\n",
    "    return str(text)\n",
    "\n",
    "def remove_html(text):\n",
    "    return BeautifulSoup(text, \"html.parser\").text\n",
    "\n",
    "def remove_whitespaces(text):\n",
    "    return str(re.sub(r' +', ' ', text))\n",
    "\n",
    "def remove_brackets(text):\n",
    "    return re.sub(r'[\\[\\](){}<>\\-_]', '', text)\n",
    "\n",
    "def remove_quotes(text):\n",
    "    return text.replace(\"'\", \"\").replace('\"', '')\n",
    "\n",
    "def remove_digits(text):\n",
    "    return ''.join([char for char in text if not char.isdigit()])\n",
    "\n",
    "def fix_contractions(text):\n",
    "    return contractions.fix(text)\n",
    "\n",
    "def remove_mf_digits(text):\n",
    "    return re.sub(r'\\b[mfMFT]+\\d+\\b|\\b\\d+[mfMFT]+\\b', '', text)\n",
    "\n",
    "def remove_special_chars(text):\n",
    "    return re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "def remove_chars_with_digits(text):\n",
    "    return re.sub(r'\\w\\d\\w', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "executionInfo": {
     "elapsed": 288,
     "status": "ok",
     "timestamp": 1692914920926,
     "user": {
      "displayName": "Oyenike Akanji",
      "userId": "06472967511729576704"
     },
     "user_tz": -60
    },
    "id": "qTzanA1erpw8"
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    if isinstance(text, str):\n",
    "        new_list = []\n",
    "        words = nltk.word_tokenize(text)\n",
    "        stopwrds = set(stopwords.words('english')) - {'not'}\n",
    "        stopwrds.update(['d', 'm', 's', 're', 've', 'll'])\n",
    "\n",
    "        for word in words:\n",
    "            if word not in stopwrds:\n",
    "                new_list.append(word)\n",
    "        return ' '.join(new_list)\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def remove_emojis(text):\n",
    "    emoj = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\"\n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"  # dingbats\n",
    "        u\"\\u3030\"\n",
    "                      \"]+\", re.UNICODE)\n",
    "    return re.sub(emoj, '', text)\n",
    "\n",
    "# Define the misspelled abbreviations dictionary\n",
    "abbr_dict = {\n",
    "    \"'cause\": \"because\",\n",
    "    \"ain't\": \"am not\",\n",
    "    \"can't\": \"can not\",\n",
    "    \"cannot\": \"can not\",\n",
    "    \"could've\": \"could have\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"doesnt\": \"does not\",\n",
    "    \"dont\": \"do not\",\n",
    "    \"gimme\": \"give me\",\n",
    "    \"gotta\": \"got to\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"hadnt\": \"had not\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"hasnt\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"havent\": \"have not\",\n",
    "    \"he'd\": \"he would\",\n",
    "    \"he'll\": \"he will\",\n",
    "    \"he's\": \"he is\",\n",
    "    \"here's\": \"here is\",\n",
    "    \"how'd\": \"how did\",\n",
    "    \"how'd'y\": \"how do you\",\n",
    "    \"how'll\": \"how will\",\n",
    "    \"how's\": \"how is\",\n",
    "    \"i'm\": \"i am\",\n",
    "    \"i'll\": \"i will\",\n",
    "    \"i've\": \"i have\",\n",
    "    \"i ve\": \"i have\",\n",
    "    \"imma\": \"i am going to\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"it'll\": \"it will\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"lemme\": \"let me\",\n",
    "    \"let's\": \"let us\",\n",
    "    \"not've\": \"not have\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"she'll\": \"she will\",\n",
    "    \"she's\": \"she is\",\n",
    "    \"that's\": \"that is\",\n",
    "    \"there's\": \"there is\",\n",
    "    \"there're\": \"there are\",\n",
    "    \"they'd\": \"they would\",\n",
    "    \"they'll\": \"they will\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"wasnt\": \"was not\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"werent\": \"were not\",\n",
    "    \"what's\": \"what is\",\n",
    "    \"what're\": \"what are\",\n",
    "    \"when's\": \"when is\",\n",
    "    \"when're\": \"when are\",\n",
    "    \"where's\": \"where is\",\n",
    "    \"where're\": \"where are\",\n",
    "    \"who's\": \"who is\",\n",
    "    \"who're\": \"who are\",\n",
    "    \"who've\": \"who have\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"would've\": \"would have\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"wouldn't've\": \"would not have\",\n",
    "    \"y'all\": \"you all\",\n",
    "    \"y'all'd\": \"you all would\",\n",
    "    \"y'all'd've\": \"you all would have\",\n",
    "    \"y'all're\": \"you all are\",\n",
    "    \"y'all've\": \"you all have\",\n",
    "    \"you'd\": \"you would\",\n",
    "    \"you'd've\": \"you would have\",\n",
    "    \"you'll\": \"you will\",\n",
    "    \"you'll've\": \"you will have\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"you've\": \"you have\"\n",
    "}\n",
    "\n",
    "def misspelled_abbreviations(text):\n",
    "    # Replace '’' with '\\'\n",
    "    text = re.sub('’', '\\'', text)\n",
    "\n",
    "    # Replace abbreviations with their full form\n",
    "    for abbr, expanded_form in abbr_dict.items():\n",
    "        text = text.replace(abbr, expanded_form)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "executionInfo": {
     "elapsed": 13680,
     "status": "ok",
     "timestamp": 1692914939464,
     "user": {
      "displayName": "Oyenike Akanji",
      "userId": "06472967511729576704"
     },
     "user_tz": -60
    },
    "id": "ZumzJ0HtrpxE"
   },
   "outputs": [],
   "source": [
    "# Remove diacritics from the 'Sentence' column using Hero library\n",
    "#hero.preprocessing.remove_diacritics(dataset.Sentence)\n",
    "\n",
    "from unidecode import unidecode\n",
    "dataset['post'] = dataset['post'].apply(unidecode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LvY2IJJAKBT9"
   },
   "outputs": [],
   "source": [
    "# Create a list of functions to apply to each row in the 'sentence' column of the DataFrame\n",
    "functions_list = [\n",
    "    convert_lowercase,\n",
    "    remove_brackets,\n",
    "    remove_chars_with_digits,\n",
    "    remove_mf_digits,\n",
    "    fix_contractions,\n",
    "    misspelled_abbreviations,\n",
    "    remove_quotes,\n",
    "    remove_url,\n",
    "    remove_newline,\n",
    "    remove_emojis,\n",
    "    remove_html,\n",
    "    remove_special_chars,\n",
    "    remove_stopwords,\n",
    "    remove_digits,\n",
    "    remove_whitespaces,\n",
    "    lemmatization\n",
    "]\n",
    "\n",
    "# Loop through each row in the 'sentence' column of the DataFrame and apply the functions in the list\n",
    "for i, line in tqdm(dataset['post'].iteritems(), total=dataset.shape[0]):\n",
    "    for func in functions_list:\n",
    "        line = func(line)\n",
    "    dataset.at[i, 'post'] = line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xQvmalYDXjZ0"
   },
   "outputs": [],
   "source": [
    "dataset.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2r2J89c2h6Eh"
   },
   "outputs": [],
   "source": [
    "#Check null values\n",
    "dataset.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-fzpo_8Pkbog"
   },
   "source": [
    "**Word Cloud showing each word importance per subreddit**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5BqG2skrkboh"
   },
   "outputs": [],
   "source": [
    "subreddit = dataset['subreddit'].unique()\n",
    "subreddit = list(subreddit)\n",
    "subreddit = list(subreddit) + list(subreddit[:3])\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(15, 15))\n",
    "\n",
    "for ax, sentiment in zip(axes.flatten(), subreddit):\n",
    "    text = \" \".join(dataset[dataset['subreddit'] == sentiment]['post'])\n",
    "    cloud = WordCloud(width=800, height=800, background_color='black', min_font_size=10).generate(text)\n",
    "    ax.imshow(cloud)\n",
    "    ax.set_title(sentiment)\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1-vgJY35sKtt"
   },
   "outputs": [],
   "source": [
    "#Save the processed data\n",
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "dataset.to_csv('/content/drive/MyDrive/Colab Notebooks/Main Dissertation/cleaned_full_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tue1S4rlmXQv"
   },
   "source": [
    "## **References**\n",
    "\n",
    "https://www.kaggle.com/code/anubhavgoyal10/spam-classifier-nlp-98-accuracy\n",
    "\n",
    "https://www.kaggle.com/code/maeshi/text-classification-with-neural-networks\n",
    "\n",
    "https://www.kaggle.com/code/mohamedabdelmohsen/emotion-analysis-and-classification-using-lstm-93\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyM6HJ9msDsYfZkSt1s1j56g",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
